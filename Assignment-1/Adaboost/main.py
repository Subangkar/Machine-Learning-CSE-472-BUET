# -*- coding: utf-8 -*-
"""AdaBoost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TZ_ksm2DHRDLDlYBLQGCnHMqQRLIneV7
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
import copy
import time

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# DATA_PATH_DRIVE='/content/drive/My Drive/Colab Notebooks/Datasets/decision-tree-dataset'
# CURRENT_PATH=`pwd`
# DATA_PATH="${CURRENT_PATH}/data"
# if [[ ! -d "${DATA_PATH}" ]]
# then
#     # echo "${DATA_PATH} not exists on your filesystem."
#     ln -s "${DATA_PATH_DRIVE}" "${DATA_PATH}"
# fi
# # /content/drive/My Drive/Colab Notebooks/Datasets/decision-tree-dataset
# # !ln -s "/gdrive/My Drive/theFolder" "/content/theFolder"
# # ls

"""###Utils"""

from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import LabelEncoder
import numpy as np
import seaborn as sn
import matplotlib.pyplot as plt
from scipy.stats import hmean


def perf_metrics_2X2(y_true, y_pred):
	cm = confusion_matrix(y_true, y_pred)
	TN = cm[0][0]
	FN = cm[1][0]
	TP = cm[1][1]
	FP = cm[0][1]

	precision = TP / (TP + FP)  # pos_pred_value
	recall = TP / (TP + FN)  # TP_rate/sensitivity
	false_discovery_rate = FP / (TP + FP)
	true_negative_rate = TN / (FP + TN)

	f1_score = hmean((precision, recall))

	# return {'Precision': precision, 'Recall': recall, 'True Negative Rate': true_negative_rate,
	#         'False Discovery Rate': false_discovery_rate, 'F1 Score': f1_score}
	return 'True Positive Rate: {:.2f}\nTrue Negative Rate: {:.2f}\nPrecision: {:.2f}\nFalse Discovery Rate: {:.2f}\nF1 ' \
	       'Score: {:.2f}\n'.format(recall, true_negative_rate, precision, false_discovery_rate, f1_score)


def plot_confusion_matrix(y_true, y_pred):
	cm = confusion_matrix(y_true, y_pred)
	sn.set(font_scale=1.4)  # for label size
	sn.heatmap(cm, annot=True, annot_kws={"size": 16}, fmt='d')
	plt.xlabel('Actual labels')
	plt.ylabel('Predicted labels')
	plt.show()

"""###Data Preprocessor"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import numpy as np


class DataFramePreprocessor:
	def __init__(self, df_train, drop_colms=None):
		if drop_colms is None:
			drop_colms = []
		self.label_encoder = LabelEncoder()
		self.one_hot_encoder = OneHotEncoder()
		self.columns_sorted = sorted([c for c in df_train.columns if c not in drop_colms])

	def trim_whitespace(self, df, columns=None):
		return DataFramePreprocessor.trim_wsp_util(df, columns=columns, inplace=True)

	def process_multi_valued_cat(self, df, multi_categorical_columns=None):
		return DataFramePreprocessor.binarization_util(df, categorical_columns=multi_categorical_columns)

	def process_numeric(self, df, numeric_columns=None):
		return DataFramePreprocessor.replace_numeric_na_util(df, numeric_columns=numeric_columns, inplace=True)

	def preprocess_test(self, df, mul_cat_colms=None, num_colms=None, bin_cat_colms=None, target_colm=None,
	                    bin_replace_map=None, num_na_symbol='', drop_colms=None):
		return self.preprocess(df, mul_cat_colms=mul_cat_colms, num_colms=num_colms, bin_cat_colms=bin_cat_colms,
		                target_colm=target_colm, bin_replace_map=bin_replace_map, num_na_symbol=num_na_symbol,
		                drop_colms=drop_colms, is_test=True, sort_index=True)

	def preprocess_train(self, df, mul_cat_colms=None, num_colms=None, bin_cat_colms=None, target_colm=None,
	                     bin_replace_map=None, num_na_symbol='', drop_colms=None):
		return self.preprocess(df, mul_cat_colms=mul_cat_colms, num_colms=num_colms, bin_cat_colms=bin_cat_colms,
		                target_colm=target_colm, bin_replace_map=bin_replace_map, num_na_symbol=num_na_symbol,
		                drop_colms=drop_colms, is_train=True, sort_index=True)

	def preprocess(self, df, mul_cat_colms=None, num_colms=None, bin_cat_colms=None, target_colm=None,
	               bin_replace_map=None, num_na_symbol='', drop_colms=None, is_train=False, is_test=False,
	               sort_index=False):
		"""

		:param df:
		:param mul_cat_colms:
		:param num_colms:
		:param bin_cat_colms:
		:param target_colm:
		:param bin_replace_map:
		:param num_na_symbol:
		:param drop_colms:
		:param is_train: set True if Train/Test pre-processing are different and df is train set
		:param is_test:  set True if Train/Test pre-processing are different and df is test set
		:param sort_index:set True if Train/Test pre-processing are different
		:return: preprocessed dataframe
		"""

		if bin_replace_map is None:
			bin_replace_map = []
		if bin_cat_colms is None:
			bin_cat_colms = []
		if num_colms is None:
			num_colms = []
		if mul_cat_colms is None:
			mul_cat_colms = []
		if drop_colms is None:
			drop_colms = []

		df.drop(columns=drop_colms, axis=1, inplace=True, errors='ignore')

		df = DataFramePreprocessor.trim_wsp_util(df)
		df = DataFramePreprocessor.binary_replace_util(df,
		                                               categorical_columns=bin_cat_colms,
		                                               replace_map=bin_replace_map)
		df = DataFramePreprocessor.binarization_util(df, categorical_columns=mul_cat_colms)
		df = DataFramePreprocessor.replace_numeric_na_util(df, numeric_columns=num_colms, na_symbol=num_na_symbol)

		if is_train and is_test:
			raise Exception('Invalid selection')
		elif is_train:
			self.columns_sorted = sorted(df.columns)
		elif is_test:
			DataFramePreprocessor.align_test_colms_util(df, self.columns_sorted)

		if sort_index:
			df.sort_index(axis=1, inplace=True)

		return df

	@staticmethod
	def trim_wsp_util(df_orig, columns=None, inplace=True):
		if not inplace:
			df = df_orig.copy(deep=True)
		else:
			df = df_orig

		if columns is None:
			columns = df.columns

		# for c in columns:
		for c in df.select_dtypes(include=['object']).columns:
			df[c] = df[c].astype(str).str.strip()

		return df

	# Out place
	@staticmethod
	def binarization_util(df, categorical_columns=None):
		"""
		:param df:
		:param categorical_columns: array of categorical columns
		:param inplace:
		:return: preprocessed df
		"""

		for c in categorical_columns:
			df = pd.concat([df, pd.get_dummies(df[c], prefix=c)], axis=1)

		df.drop(columns=categorical_columns, axis=1, inplace=True, errors='ignore')

		return df

	@staticmethod
	def replace_numeric_na_util(df_orig, numeric_columns=None, inplace=True, na_symbol=''):
		"""
		:param df_orig:
		:param numeric_columns: array of numeric columns
		:param na_symbol: string value which denotes that value is missing
		:param inplace:
		:return: preprocessed df
		"""
		if not inplace:
			df = df_orig.copy(deep=True)
		else:
			df = df_orig

		for c in numeric_columns:
			df[c] = df[c].replace(to_replace=na_symbol, value='')
			# df[c] = df[c].replace(to_replace='[ ]+', value='', regex=True)
			df[c] = pd.to_numeric(df[c], errors='ignore')
			df[c].fillna(value=df[c].mean(), inplace=True)

		return df

	@staticmethod
	def binary_replace_util(df_orig, categorical_columns=None, replace_map=None, inplace=True):
		"""
		:param df_orig:
		:param categorical_columns: array of categorical columns
		:param replace_map: categorical_columns's corresponding array of map
		:param inplace:
		:return: preprocessed df
		"""
		if len(categorical_columns) != len(replace_map):
			raise Exception('Binary cat colms & map len mismatch')

		if not inplace:
			df = df_orig.copy(deep=True)
		else:
			df = df_orig

		for c, m in zip(categorical_columns, replace_map):
			df[c].replace(m, inplace=inplace)

		return df

	@staticmethod
	def align_test_colms_util(df, train_colms):
		test_colms = df.columns
		colms_missing = [c for c in train_colms if c not in test_colms]
		colms_extras = [c for c in test_colms if c not in train_colms]
		for colm in colms_missing:
			df[colm] = np.zeros(df.shape[0])
		df.drop(columns=colms_extras, axis=1, inplace=True, errors='ignore')

"""###Decision Tree Classifier"""

import collections
from datetime import datetime

import numpy as np

# from utils import perf_metrics_2X2, plot_confusion_matrix


class DtUtils:

	@staticmethod
	def resample(X, y, k=None, sample_weight=None, random_state=None):
		if X.shape[0] != y.shape[0]:
			raise Exception('dimension mismatch')
		if k is None:
			k = X.size
		if sample_weight is None:
			sample_weight = np.full((k, 1), 1 / k)
		if random_state is not None:
			np.random.seed(random_state)
		choices = np.random.choice(X.shape[0], size=k, p=sample_weight)
		return X[choices], y[choices]

	# y = {0,1}
	@staticmethod
	def entropy(y):
		# N = len(y)
		N = y.shape[0]
		s1 = (y == 1).sum()
		if 0 == s1 or N == s1:
			return 0
		p1 = float(s1) / N
		p0 = 1 - p1
		return -p0 * np.log2(p0) - p1 * np.log2(p1)

	# y = {0,1}
	"""
	:returns (information_gain, entropy_left, entropy_right)
	"""

	@staticmethod
	def information_gain(x_parent, y_parent, split, entropy_parent=None):
		if entropy_parent is None:
			entropy_parent = DtUtils.entropy(y_parent)

		y_child_le = y_parent[x_parent <= split]
		y_child_gt = y_parent[x_parent > split]
		N = y_parent.shape[0]
		N_le = y_child_le.shape[0]

		if N_le == 0:
			return 0, 0, entropy_parent
		if N_le == N:
			return 0, entropy_parent, 0

		p_le = float(N_le) / N
		p_gt = 1 - p_le

		entropy_child_le = DtUtils.entropy(y_child_le)
		entropy_child_gt = DtUtils.entropy(y_child_gt)

		info_gain = entropy_parent - p_le * entropy_child_le - p_gt * entropy_child_gt

		return info_gain, entropy_child_le, entropy_child_gt

	@staticmethod
	def find_split(X, y, column, entropy_parent=None):
		# Binarization -------------------
		sort_idx = np.argsort(X[:, column])
		x_values = X[sort_idx, column]
		y_values = y[sort_idx]
		split_indices = np.nonzero(y_values[1:] != y_values[:-1])[0]
		split_values = np.unique((x_values[split_indices] + x_values[split_indices + 1]) / 2)
		# --------------------------------

		(max_ig, entropy_left, entropy_right), split_thresh = max(map(lambda split_value: (
			DtUtils.information_gain(x_values, y_values, split_value, entropy_parent=entropy_parent), split_value),
		                                                              split_values), key=lambda v: (v[0][0], v[1]))

		return max_ig, split_thresh, entropy_left, entropy_right

	@staticmethod
	def split_dataset(X, y, feature, split_thresh):
		X_left, X_right = X[X[:, feature] <= split_thresh], X[X[:, feature] > split_thresh]
		y_left, y_right = y[X[:, feature] <= split_thresh], y[X[:, feature] > split_thresh]

		return X_left, X_right, y_left, y_right


class TreeNode:
	def __init__(self, depth=0, max_depth=None):
		self.depth = depth
		self.max_depth = max_depth

		self.feature = None
		self.split_threshold = None
		self.prediction = None

		self.left = None
		self.right = None

		self.entropy = None

	def make_terminal_node(self, X, y):
		self.feature = None
		self.split_threshold = None
		self.left = self.right = None
		self.prediction = collections.Counter(y).most_common()[0][0]

	def make_cutoff_node(self, X, y, feature, split_thresh):
		self.feature = feature
		self.split_threshold = split_thresh
		self.left = self.right = None

		self.prediction = (collections.Counter(y[X[:, feature] <= split_thresh]).most_common()[0][0],
		                   collections.Counter(y[X[:, feature] > split_thresh]).most_common()[0][0])

	def buildtree(self, X, y, depth, max_depth=None):
		# print(depth, end=' ')
		# self.entropy = DtUtils.entropy(y)
		if len(y) == 1 or self.entropy == 0:
			self.make_terminal_node(X, y)
			return

		best_feature, max_ig, best_split_thresh, entropy_left, entropy_right = max(
			map(lambda feature: ((feature,) + (DtUtils.find_split(X, y, column=feature, entropy_parent=self.entropy))),
			    range(X.shape[1])), key=lambda v: v[1])

		if max_ig <= 0:
			self.make_terminal_node(X, y)
			return

		if max_depth is not None and depth >= max_depth:
			self.make_cutoff_node(X, y, feature=best_feature, split_thresh=best_split_thresh)
			return

		self.feature = best_feature
		self.split_threshold = best_split_thresh

		X_left, X_right, y_left, y_right = DtUtils.split_dataset(X, y, best_feature, best_split_thresh)

		self.left = TreeNode(self.depth + 1, self.max_depth)
		self.right = TreeNode(self.depth + 1, self.max_depth)

		self.left.entropy = entropy_left
		self.right.entropy = entropy_right

		self.left.buildtree(X=X_left, y=y_left, depth=depth + 1, max_depth=max_depth)
		self.right.buildtree(X=X_right, y=y_right, depth=depth + 1, max_depth=max_depth)

	def predict_val(self, x):
		if self.feature is None:
			return self.prediction
		if x[self.feature] <= self.split_threshold:
			return self.left.predict_val(x) if self.left is not None else self.prediction[0]
		else:
			return self.right.predict_val(x) if self.right is not None else self.prediction[1]

	def predict(self, X):
		return np.array(list(map(lambda x: self.predict_val(x), X)))

	@staticmethod
	def print_tree(node, depth):
		if node is None:
			return
		if node.feature is None:
			print('     ' * depth + 'y={:d}'.format(node.prediction) + ' ent=' + '{:.3g}'.format(node.entropy))
		elif node.feature is not None and (node.left is None and node.right is None):
			print('     ' * depth + '(y={:d})'.format(node.prediction[0]) + ' X[{:2d}]<={:.3g}'.format(
				node.feature, node.split_threshold) + ' (y={:d})'.format(
				node.prediction[1]) + ' ent=' + '{:.3g}'.format(
				node.entropy))
		else:
			TreeNode.print_tree(node.left, depth + 1)
			print('     ' * depth + 'X[{:2d}]<={:.3f}'.format(node.feature,
			                                                  node.split_threshold) + ' ent=' + '{:.3g}'.format(
				node.entropy))
			TreeNode.print_tree(node.right, depth + 1)


class DecisionTree:
	def __init__(self, max_depth=None):
		self.max_depth = max_depth

		self.root = None

	def fit(self, X, y, sample_weight=None, random_state=None):
		X = np.array(X)
		y = np.array(y)
		if sample_weight is not None:
			X, y = DtUtils.resample(X, y, k=sample_weight.shape[0], sample_weight=sample_weight,
			                        random_state=random_state)
		self.root = TreeNode(depth=0, max_depth=self.max_depth)
		self.root.entropy = DtUtils.entropy(y)
		self.root.buildtree(X=X, y=y, depth=0, max_depth=self.max_depth)

	def predict(self, X):
		return self.root.predict(np.array(X))

	def score(self, X, y):
		y_p = self.predict(X)
		return np.mean(y_p == y)

	def report(self, X, y):
		# return classification_report(np.array(y), self.predict(X), target_names=['class 0', 'class 1'])
		return perf_metrics_2X2(y_true=np.array(y), y_pred=self.predict(X))

	def plot_cm(self, X, y):
		plot_confusion_matrix(y_true=y, y_pred=self.predict(X))

	def print_tree(self):
		TreeNode.print_tree(self.root, depth=0)

"""###AdaBoost Classifier"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder
import copy

# from models.decisiontree import DecisionTree
# from utils import perf_metrics_2X2


class AdaBoost:
	def __init__(self, n_estimators=None, base_estimator=None):
		if n_estimators is None:
			n_estimators = 5
		if base_estimator is None:
			base_estimator = DecisionTree(max_depth=1)

		self.n_estimators = n_estimators
		self.classifier = base_estimator

		self.n_classes = 0
		self.classes = None
		self.class_binary_encoded = None

		self.estimators = []
		self.estimator_weights = []

	def fit(self, X, y, eps=1E-12, random_state=None):
		"""
		Only for binary classifier
		:param random_state: seed for random sampling
		:param X: feature vectors
		:param y: target vector
		:param eps: precision to avoid division by 0
		"""
		N, _ = X.shape
		W = np.ones(N) / N

		self.classes = np.unique(y)
		self.n_classes = self.classes.size
		self.class_binary_encoded = OneHotEncoder(categories=self.classes, sparse=False)

		if random_state is not None:
			np.random.seed(random_state)

		print('fitting ' + str(self.n_estimators) + ' models')
		for k in range(self.n_estimators):
			estimator = copy.copy(self.classifier)
			estimator.fit(X, y, sample_weight=W, random_state=None)
			y_p = estimator.predict(X)

			error = W.dot(y_p != y)  # error = sum(w[j]) if y_p_j != y_j

			if error > 0.5:
				continue

			if error == 0:
				error = eps

			# PDF
			W[y_p == y] *= (error / (1 - error))
			W /= W.sum()

			estimator_weight = np.log(1 - error) - np.log(error)
			# ------------------------------------------

			# https://towardsdatascience.com/boosting-algorithm-adaboost-b6737a9ee60c
			# estimator_weight = 0.5 * (np.log(1 - error) - np.log(error))
			#
			# W = W * np.exp(-estimator_weight * y * y_p)
			# W = W / W.sum()
			# -----------------------------------------------------------------------

			self.estimators.append(estimator)
			self.estimator_weights.append(estimator_weight)

	def predict(self, X):
		N, _ = X.shape
		y_val = np.zeros(N)
		for estimator_weight, estimator in zip(self.estimator_weights, self.estimators):
			y_p = estimator.predict(X)
			y_p[y_p == 0] = -1
			y_val += estimator_weight * y_p
		return (np.sign(y_val) == 1).astype(int)  # , y_val

	# class_probs = np.zeros(shape=(N, self.n_classes))
	# for estimator_weight, estimator in zip(self.estimator_weights, self.estimators):
	# 	y_p = estimator.predict(X)
	# 	# y_p_reshaped = y_p.reshape(-1, 1)
	# 	feature_select_vect = self.class_binary_encoded.fit_transform(y_p.reshape(-1, 1))
	# 	class_probs += estimator_weight * feature_select_vect

	def score(self, X, y):
		return np.mean(self.predict(X) == y)

	def report(self, X, y):
		# return classification_report(np.array(y), self.predict(X), target_names=['class 0', 'class 1'])
		return perf_metrics_2X2(y_true=np.array(y), y_pred=self.predict(X))

	def plot_cm(self, X, y):
		plot_confusion_matrix(y_true=y, y_pred=self.predict(X))

"""###Model Runner"""

def run_adaboost(dataset_name, dataset, K=None, random_state=None):
	if K is None:
		K = [5, 10, 15, 20]
	X_train, X_test, y_train, y_test = dataset
	for k in K:
		model = AdaBoost(n_estimators=k, base_estimator=DecisionTree(max_depth=1))
		st_time = time.time()
		model.fit(X_train, y_train, random_state=random_state)
		en_time = time.time()
		print(dataset_name, ' AdaBoost x', k, ' Train:', '{:.4f}'.format(model.score(X_train, y_train)),
		      ' Test:', '{:.4f}'.format(model.score(X_test, y_test)))
		# model.plot_cm(X_test, y_test)
		print('elaspled time: ', '{:.4f}'.format(en_time - st_time))
	print(flush=True)


def run_decisionTree(dataset_name, dataset, random_state=None):
	X_train, X_test, y_train, y_test = dataset
	model = DecisionTree()
	st_time = time.time()
	model.fit(X_train, y_train, random_state=random_state)
	en_time = time.time()
	print(dataset_name, ' Decision Tree', ' Train:', '{:.4f}'.format(model.score(X_train, y_train)),
	      ' Test:', '{:.4f}'.format(model.score(X_test, y_test)))
	print('Train:\n' + model.report(X_train, y_train))
	print('Test:\n' + model.report(X_test, y_test))
	# model.plot_cm(X_test, y_test)
	print('elaspled time: ', '{:.4f}'.format(en_time - st_time))
	print(flush=True)

"""###Dataset

####Telco
"""

def train_test_dataset_telco(project_root='./', random_state=None):
	df = pd.read_csv(project_root + 'data/WA_Fn-UseC_-Telco-Customer-Churn.csv')

	no_int_colms = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection',
	                'TechSupport', 'StreamingTV', 'StreamingMovies']

	for c in no_int_colms:
		df[c].replace(to_replace='No internet service', value='No', inplace=True)
	df['MultipleLines'].replace(to_replace='No phone service', value='No', inplace=True)

	yes_no_colms = ['Partner', 'Dependents', 'PhoneService', 'MultipleLines'] \
	               + no_int_colms + ['PaperlessBilling', 'Churn']

	mult_val_colms = ['InternetService', 'Contract', 'PaymentMethod']

	num_colms = ['TotalCharges']
	df = DataFramePreprocessor(df_train=df).preprocess(df,
	                                                   drop_colms=['customerID'],
	                                                   mul_cat_colms=mult_val_colms,
	                                                   num_colms=num_colms,
	                                                   target_colm='Churn',
	                                                   bin_cat_colms=yes_no_colms + ['gender'],
	                                                   bin_replace_map=[{'Yes': 1, 'No': 0}] * len(yes_no_colms) + [
		                                                   {'Male': 1, 'Female': 0}])

	X, y = df.drop(columns=['Churn']).to_numpy(), df['Churn'].to_numpy()
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)

	# plt.figure(figsize = (24,20))
	# sns.heatmap(df.corr())

	return X_train, X_test, y_train, y_test

"""####Adult"""

# %%
import pandas as pd
import numpy as np
import seaborn as sn

# %%
from sklearn.metrics import confusion_matrix

column_labels = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation',
                 'relationship', 'race', 'sex', 'capital - gain', 'capital - loss', 'hours - per - week',
                 'native - country', 'salary']

numeric_colms = ['age', 'fnlwgt', 'education-num', 'capital - gain', 'capital - loss', 'hours - per - week']
yes_no_colms = ['sex', 'salary']
cat_colms = [cat for cat in column_labels[:-1] if cat not in numeric_colms]
cat_colms = [cat for cat in cat_colms if cat not in yes_no_colms]

"""
0  age: continuous.
1  workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.
2  fnlwgt: continuous.
3  education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.
4  education-num: continuous.
5  marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.
6  occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.
7  relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.
8  race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.
9  sex: Female, Male.
10 capital-gain: continuous.
11 capital-loss: continuous.
12 hours-per-week: continuous.
13 native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.

14 salary: <=50K, >50K
"""


def train_test_dataset_adult(project_root='./', random_state=None):
	df = pd.read_csv(project_root + 'data/adult/adult.data', header=None, names=column_labels)
	df_test = pd.read_csv(project_root + 'data/adult/adult.test', skiprows=1, header=None, names=column_labels)

	pr = DataFramePreprocessor(df_train=df)

	df_ = pr.preprocess_train(df,
	                          bin_cat_colms=yes_no_colms,
	                          bin_replace_map=[{'Male': 1, 'Female': 0}, {'>50K': 1, '<=50K': 0}],
	                          num_colms=numeric_colms,
	                          mul_cat_colms=cat_colms,
	                          num_na_symbol='?')

	df_test_ = pr.preprocess_test(df_test,
	                              bin_cat_colms=yes_no_colms,
	                              bin_replace_map=[{'Male': 1, 'Female': 0}, {'>50K.': 1, '<=50K.': 0}],
	                              num_colms=numeric_colms,
	                              mul_cat_colms=cat_colms,
	                              num_na_symbol='?')


	X_train, y_train = df_.drop(columns=['salary']).to_numpy(), df_['salary'].to_numpy()
	X_test, y_test = df_test_.drop(columns=['salary']).to_numpy(), df_test_['salary'].to_numpy()

	return X_train, X_test, y_train, y_test

"""####Credit Card Fraud"""

import pandas as pd
from sklearn.model_selection import train_test_split


def train_test_dataset_credit(n_neg_samples=20000, project_root='./', random_state=None):
	df = pd.read_csv(project_root + 'data/creditcard.csv')
	df.drop(columns=['Time'], axis=0, inplace=True)

	if n_neg_samples is not None:
		df_pos = df[df.Class == 1].reset_index()
		df.drop(df[df.Class == 1].index, inplace=True)
		df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)

		df = pd.concat([df.head(n_neg_samples), df_pos], axis=0)
		df = df.sample(frac=1, random_state=random_state).reset_index(drop=True)
		df.drop(columns=['index'], axis=0, inplace=True)

	X, y = df.drop(columns=['Class']).to_numpy(), df['Class'].to_numpy()
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)

	return X_train, X_test, y_train, y_test

"""####Imports"""

RANDOM_STATE=15

datasets = dict()
datasets['telco'] = train_test_dataset_telco(project_root='./', random_state=RANDOM_STATE)
datasets['adult'] = train_test_dataset_adult(project_root='./', random_state=RANDOM_STATE)
datasets['crdit'] = train_test_dataset_credit(project_root='./', random_state=RANDOM_STATE)

"""###Run"""

for (k, v) in datasets.items():
	run_decisionTree(k, v, random_state=RANDOM_STATE)

	run_adaboost(k, v, random_state=RANDOM_STATE)

"""### Print"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# !apt-get install texlive texlive-xetex texlive-latex-extra pandoc
# !pip install pypandoc
# !cp "/content/drive/My Drive/Colab Notebooks/AdaBoost.ipynb" ./
# !jupyter nbconvert --to HTML "AdaBoost.ipynb"